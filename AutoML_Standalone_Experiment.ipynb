{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6708efea-c36a-466e-bb73-fde2cef02377",
   "metadata": {},
   "source": [
    "# Azure Machine Learning - AutoML Experiment using v2 SDK\n",
    "\n",
    "Sample here can be run from an Azure ML Compute Instance using the Python 3.10 - SDK v2 Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123106c-a148-463c-9f9f-df107e5f7e89",
   "metadata": {},
   "source": [
    "### Retrieve source data from [GH Repo](https://github.com/ignavinuales/Battery_RUL_Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2ec54-6130-41d7-8f72-31a8692f0f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the GitHub repository containing the Battery RUL Prediction project into the Jupyter notebook's workspace  \n",
    "! git clone https://github.com/ignavinuales/Battery_RUL_Prediction  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a8603b-b278-4691-8a3f-7e9955c6967e",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec11059-0069-4f9e-82a0-36c048a7f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages and modules  \n",
    "from azure.ai.ml import MLClient, Input, automl\n",
    "from azure.ai.ml.entities import Data, AmlCompute\n",
    "from azure.ai.ml.constants import AssetTypes  \n",
    "from azure.ai.ml import automl\n",
    "from azure.identity import DefaultAzureCredential  \n",
    "import mltable  \n",
    "import pandas as pd    \n",
    "import numpy as np    \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7bcfa5-15ad-4022-8cf9-3acd87f8a702",
   "metadata": {},
   "source": [
    "### Instantiate ml_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db43ec-f16b-443d-ab00-3509aed027d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLClient object from configuration file and DefaultAzureCredential  \n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential())  \n",
    "\n",
    "# Print the MLClient object  \n",
    "print(ml_client)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb8f05-54e3-4920-a019-b9a7591d7a49",
   "metadata": {},
   "source": [
    "### Create compute cluster for AutoML training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00683c2f-d786-4576-8269-14af63586d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the CPU compute target to be used or created  \n",
    "cpu_compute_target = \"cpu-cluster\"  \n",
    "  \n",
    "try:  \n",
    "    # Try to get the details of the specified compute target if it already exists  \n",
    "    ml_client.compute.get(cpu_compute_target)  \n",
    "except Exception:  \n",
    "    # If the compute target does not exist, catch the exception and create a new one  \n",
    "    print(\"Creating a new cpu compute target...\")  \n",
    "    # Define the compute target specifications such as name, VM size, and instance limits  \n",
    "    compute = AmlCompute(  \n",
    "        name=cpu_compute_target, size=\"STANDARD_D2_V2\", min_instances=0, max_instances=4  \n",
    "    )  \n",
    "    # Initiate the creation or update of the compute target and wait for the operation to complete  \n",
    "    ml_client.compute.begin_create_or_update(compute).result()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537de220-0864-4864-b6e2-c5b54aa7130d",
   "metadata": {},
   "source": [
    "### Split data into train/validation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effdc309-47bb-4a11-ad73-45c08e6e948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in downloaded Battery Life directory that contain 'features' in their name  \n",
    "data_dir = './Battery_RUL_Prediction/Datasets/HNEI_Processed'  \n",
    "files = [os.path.join(data_dir, x) for x in os.listdir(data_dir) if 'features' in x]  \n",
    "  \n",
    "# Randomly select 2 files for validation and use the rest for training  \n",
    "validation_files = random.sample(files, 2)  \n",
    "train_files = [x for x in files if x not in validation_files]  \n",
    "  \n",
    "# Initialize an empty DataFrame for validation data  \n",
    "validation_df = pd.DataFrame()  \n",
    "# Loop through each file in the validation set, read the data, and concatenate it into the validation DataFrame  \n",
    "for f in validation_files:  \n",
    "    validation_df = pd.concat([validation_df, pd.read_csv(f)])  \n",
    "# Drop unnecessary columns from the validation DataFrame  \n",
    "validation_df = validation_df.drop(columns=['Unnamed: 0', 'Cycle_Index'])  \n",
    "  \n",
    "# Initialize an empty DataFrame for training data  \n",
    "train_df = pd.DataFrame()  \n",
    "# Loop through each file in the training set, read the data, and concatenate it into the training DataFrame  \n",
    "for f in train_files:  \n",
    "    train_df = pd.concat([train_df, pd.read_csv(f)])  \n",
    "# Drop unnecessary columns from the training DataFrame  \n",
    "train_df = train_df.drop(columns=['Unnamed: 0', 'Cycle_Index'])  \n",
    "  \n",
    "# Display the first few rows of the training DataFrame  \n",
    "print(train_df.head())  \n",
    "  \n",
    "# Create directories for storing processed validation and training data if they don't exist  \n",
    "os.makedirs('./validation', exist_ok=True)  \n",
    "os.makedirs('./train', exist_ok=True)  \n",
    "  \n",
    "# Save the processed validation and training DataFrames to CSV files  \n",
    "validation_df.to_csv('./validation/validation_data.csv', index=False)  \n",
    "train_df.to_csv('./train/train_data.csv', index=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192bfe93-db7e-4711-a854-578c3b8cf37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name for the validation dataset  \n",
    "validation_dataset_name = 'VALIDATION_BatteryCycles'  \n",
    "  \n",
    "# Provide a description for the validation dataset  \n",
    "validation_dataset_description = 'Holdout dataset from battery cycle repo @https://github.com/ignavinuales/Battery_RUL_Prediction'  \n",
    "  \n",
    "# Specify the local path to save the validation dataset  \n",
    "validation_tbl_path = './validation'  \n",
    "  \n",
    "# Define the file path to the validation dataset CSV file  \n",
    "validation_paths = [{'file':'./validation/validation_data.csv'}]  \n",
    "  \n",
    "# Load the validation dataset from the CSV file into a MLTable  \n",
    "validation_tbl = mltable.from_delimited_files(paths=validation_paths)  \n",
    "  \n",
    "# Save the MLTable to the specified path  \n",
    "validation_tbl.save(validation_tbl_path)  \n",
    "  \n",
    "# Create a Data object for the validation dataset with properties such as path, type, description, and name  \n",
    "validation_data = Data(  \n",
    "    path=validation_tbl_path,    \n",
    "    type=AssetTypes.MLTABLE,    \n",
    "    description=validation_dataset_description,    \n",
    "    name=validation_dataset_name,    \n",
    ")  \n",
    "  \n",
    "# Create or update the validation dataset in Azure ML workspace  \n",
    "validation_dataset = ml_client.data.create_or_update(validation_data)  \n",
    "  \n",
    "# Retrieve the validation data asset information using the dataset name and version  \n",
    "validation_data_asset = ml_client.data.get(name=validation_dataset_name, version=validation_dataset.version)  \n",
    "  \n",
    "# Load the validation data asset into a pandas DataFrame and display the first few rows  \n",
    "mltable.load(f'azureml:/{validation_data_asset.id}').to_pandas_dataframe().head()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802d447-7fa5-40e2-96bf-14af01c955bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name for the training dataset  \n",
    "train_dataset_name = 'TRAIN_BatteryCycles'  \n",
    "  \n",
    "# Provide a description for the training dataset  \n",
    "train_dataset_description = 'Training dataset from battery cycle repo @https://github.com/ignavinuales/Battery_RUL_Prediction'  \n",
    "  \n",
    "# Specify the local path to save the training dataset  \n",
    "train_tbl_path = './train'  \n",
    "  \n",
    "# Define the file path to the training dataset CSV file  \n",
    "train_paths = [{'file':'./train/train_data.csv'}]  \n",
    "  \n",
    "# Load the training dataset from the CSV file into a MLTable  \n",
    "train_tbl = mltable.from_delimited_files(paths=train_paths)  \n",
    "  \n",
    "# Save the MLTable to the specified path  \n",
    "train_tbl.save(train_tbl_path)  \n",
    "  \n",
    "# Create a Data object for the training dataset with properties such as path, type, description, and name  \n",
    "train_data = Data(  \n",
    "    path=train_tbl_path,    \n",
    "    type=AssetTypes.MLTABLE,    \n",
    "    description=train_dataset_description,    \n",
    "    name=train_dataset_name,    \n",
    ")  \n",
    "  \n",
    "# Create or update the training dataset in Azure ML workspace  \n",
    "train_dataset = ml_client.data.create_or_update(train_data)  \n",
    "  \n",
    "# Retrieve the training data asset information using the dataset name and version  \n",
    "train_data_asset = ml_client.data.get(name=train_dataset_name, version=train_dataset.version)  \n",
    "  \n",
    "# Load the training data asset into a pandas DataFrame and display the first few rows  \n",
    "mltable.load(f'azureml:/{train_data_asset.id}').to_pandas_dataframe().head()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623190b1-b479-4673-a7b2-0b841874702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input data for the training job using the Azure ML dataset identifier for the training dataset  \n",
    "training_input = Input(path=f'azureml:{train_dataset_name}:{train_dataset.version}')  \n",
    "  \n",
    "# Define the input data for the validation during training using the Azure ML dataset identifier for the validation dataset  \n",
    "validation_input = Input(path=f'azureml:{validation_dataset_name}:{validation_dataset.version}')  \n",
    "  \n",
    "# Configure the AutoML regression job with required parameters like compute target, experiment name, training and validation data  \n",
    "# Also, set the target column name for prediction, the primary metric to evaluate models, and enable model explainability  \n",
    "regression_job = automl.regression(  \n",
    "    compute=cpu_compute_target,  \n",
    "    experiment_name='Battery_Cycle_RUL_Prediction',  \n",
    "    training_data=training_input,  \n",
    "    validation_data=validation_input,  \n",
    "    target_column_name=\"RUL\",  \n",
    "    primary_metric=\"r2_score\",  \n",
    "    enable_model_explainability=True,  \n",
    ")  \n",
    "  \n",
    "# Set limits for the regression job such as total time, time per trial, maximum number of trials, and enable early termination  \n",
    "regression_job.set_limits(  \n",
    "    timeout_minutes=600,   \n",
    "    trial_timeout_minutes=20,   \n",
    "    max_trials=15,  \n",
    "    enable_early_termination=True,  \n",
    ")  \n",
    "  \n",
    "# Set training properties, in this case enabling ONNX compatible models which can be beneficial for cross-platform consistency  \n",
    "regression_job.set_training(  \n",
    "    enable_onnx_compatible_models=True  \n",
    ")  \n",
    "  \n",
    "# Set the featurization mode to 'auto' allowing AutoML to handle feature engineering automatically  \n",
    "regression_job.set_featurization(  \n",
    "    mode='auto'  \n",
    ")  \n",
    "  \n",
    "# Submit the configured AutoML job to the Azure ML workspace and print the job details  \n",
    "returned_job = ml_client.jobs.create_or_update(  \n",
    "    regression_job  \n",
    ")  \n",
    "  \n",
    "print(f\"Created job: {returned_job}\")  \n",
    "  \n",
    "# Retrieve and print the URL for monitoring the status of the submitted job in Azure Machine Learning Studio  \n",
    "returned_job.services[\"Studio\"].endpoint  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
