{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea1a365-bed0-45bd-9a17-72a28137e8fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries  \n",
    "import pandas as pd  \n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c3aad1-70d4-4569-95cd-52aed1e776e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# Install Git if it's not already installed\n",
    "apt-get install -y git\n",
    "\n",
    "# Clone the repository (replace with the URL of your repository)\n",
    "git clone https://github.com/ignavinuales/Battery_RUL_Prediction  /tmp/battery\n",
    "\n",
    "# Move to the directory where you cloned your repository to check its contents\n",
    "cd /tmp/repository\n",
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd46ce5-5aee-4350-9686-430eedcb9fdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "feature_data = [x for x in os.listdir('/tmp/battery/Datasets/HNEI_Processed') if 'features' in x]\n",
    "feature_data = [os.path.join('/tmp/battery/Datasets/HNEI_Processed', x) for x in feature_data]\n",
    "feature_data\n",
    "\n",
    "# Initialize a SparkSession with Delta support  \n",
    "spark = SparkSession.builder.appName(\"DeltaTableExample\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\").config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in feature_data:\n",
    "    print(file)\n",
    "    new_df = pd.read_csv(file)\n",
    "    new_df = new_df.drop(columns=['Unnamed: 0'])\n",
    "    new_df['Source'] = file.split('/')[-1]\n",
    "    df = pd.concat([df, new_df])\n",
    "    \n",
    "\n",
    "def remove_non_alphanumeric_chars(input_str):\n",
    "    # Replace all whitespaces with underscores\n",
    "    input_str = input_str.replace(' ', '_')\n",
    "    \n",
    "    # Use a list comprehension to filter out non-alphanumeric characters (excluding underscore)\n",
    "    only_alphanumeric_and_underscore = [char for char in input_str if char.isalnum() or char == '_']\n",
    "    \n",
    "    # Join the characters back into a single string\n",
    "    result_str = ''.join(only_alphanumeric_and_underscore)\n",
    "    \n",
    "    return result_str\n",
    "\n",
    "new_cols = {}\n",
    "for col in df.columns:\n",
    "    new_cols[col] = remove_non_alphanumeric_chars(col)\n",
    "\n",
    "df = df.rename(columns=new_cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db58648d-da76-4cd6-b32b-1357be04d9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b29538d9-48a8-4afb-82b0-32007ed3aa9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e57450-0158-4c01-8fcc-35f7a9552396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up the configuration to access your Azure Data Lake Storage Gen2 account\n",
    "spark.conf.set(\"fs.azure.account.auth.type.<YOUR-ACCOUNT>.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.<YOUR-ACCOUNT>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.<YOUR-ACCOUNT>.dfs.core.windows.net\", \"<CLIENT-ID>\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.<YOUR-ACCOUNT>.dfs.core.windows.net\",\"<CLIENT-SECRET>\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<YOUR-ACCOUNT>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<TENANT-ID>/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e1f18f-ee50-4ad5-abd3-32ead238cea7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path where the Delta table will be stored in Azure Data Lake Storage Gen2\n",
    "delta_table_path = \"abfss://mldata@<YOUR-ACCOUNT>.dfs.core.windows.net/battery_cycle_rul_data\"\n",
    "\n",
    "# Write the Spark DataFrame as a Delta table to Azure Data Lake Storage Gen2\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "# Register the Delta table in the metastore (optional)\n",
    "spark.sql(f\"CREATE TABLE my_delta_table USING DELTA LOCATION '{delta_table_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f10909-2d67-48ef-ae17-de4e25b525ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hold_df = spark.sql(\"SELECT * FROM my_delta_table\")\n",
    "display(hold_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4040565877805009,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta_Loading_DBX",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
